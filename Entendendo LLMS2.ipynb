{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nvidia.com/pt-br/data-center/tesla-t4/\n",
    "!nvidia-smi # <-- visualizando informacoes do drive de video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# não precisamos instalar nada de ML/IA/DL no colab, já vem instalado!\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline --> cuida de todo o pre proc (tokenizar e codificar) do texto\n",
    "bumblebee = transformers.pipeline(\n",
    "    task='text-generation',\n",
    "    model='pierreguillou/gpt2-small-portuguese',\n",
    "    device='cuda' # vamos rodar com cuda-cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bumblebee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cada coluna de imput é uma palavra! o vocabulário do modelo é o número de tokens\n",
    "# que ele suporta na entrada (primeira das primeiras camadas de embedding)\n",
    "\n",
    "# Embedding(50257, 768) --> conhece 50257 palavras, suporta textos com até 768\n",
    "\n",
    "bumblebee.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisa_prompt(model, prompt):\n",
    "\n",
    "  # precisamos acessar especificamente o tokenizer\n",
    "  # (obs: pt --> pytorch, ou seja, o backend sendo usado). Existem 2 backends classicos\n",
    "  # pytorch (pt) e o tensorflow (tf)\n",
    "  token_ids = model.tokenizer.encode(prompt, return_tensors='pt')\n",
    "  tokens    = model.tokenizer.convert_ids_to_tokens(token_ids[0])\n",
    "\n",
    "  for palavra, token, token_id in zip(prompt.split(' '), tokens, token_ids[0]):\n",
    "    print(\"-\", palavra, token, token_id)\n",
    "\n",
    "  print(f\"total de tokens: {len(token_ids[0])}\")\n",
    "\n",
    "  output = model(prompt)\n",
    "  print(output)\n",
    "\n",
    "  return\n",
    "\n",
    "prompts_alucinacoes = [\n",
    "    \"Quem foi o presidente do Brasil em 1800?\",\n",
    "    \"Qual é a cor do vento?\",\n",
    "    \"Qual é a receita secreta da Coca-Cola?\",\n",
    "    \"Me conte a biografia do Rei Arthur, o fundador dos Estados Unidos.\",\n",
    "    \"Como funciona o motor de dobra espacial usado nos foguetes da NASA?\",\n",
    "    \"Qual é o nome do país invisível entre a França e a Alemanha?\",\n",
    "    \"Descreva os últimos avanços em viagens no tempo anunciados pela ONU.\",\n",
    "    \"Qual é a opinião oficial da Organização Mundial da Saúde sobre a vida em Marte?\",\n",
    "    \"Como era a internet na época dos dinossauros?\",\n",
    "    \"Quais foram os impactos das redes sociais na vida dos faraós do Egito?\"\n",
    "]\n",
    "\n",
    "analisa_prompt(bumblebee, prompts_alucinacoes[5])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
