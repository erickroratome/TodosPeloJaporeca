{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vendo se temos uma GPU:\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# pedra, Pedra\n",
    "\n",
    "# pipeline serve para agrupar todas as funções necessárias da tarefa\n",
    "# para processar a entrada e gerar a saída\n",
    "gerbo = transformers.pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device=\"cuda\" # cpu, cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo escolhinho: 22 blocos de processamento\n",
    "# faz normalização\n",
    "# suporta até 2048 tokens de input\n",
    "# embedding de 32000 tokens: ele conhece 32k palavras\n",
    "# tarefa dele: recebe um input (texto que será codificado em tokens com o tokenizer),\n",
    "# e retorna a probabilidade de cada uma das 32k palavras de ser a próxima\n",
    "\n",
    "gerbo.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisa_e_processa_prompt(modelo, prompt):\n",
    "  print(\"=\"*80)\n",
    "\n",
    "  # fazendo o processo normal --- codificando\n",
    "  tokens = modelo.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "  print(f\"- prompt original: {prompt}\")\n",
    "  print(f\"- token ids:{tokens}\")\n",
    "  print(f\"- # de tokens:{len(tokens)}\")\n",
    "\n",
    "  # fazendo o processo reverso --- decodificando\n",
    "  decoded = modelo.tokenizer.convert_ids_to_tokens(tokens[0])\n",
    "  print(f\"- tokens decodificados: {decoded}\")\n",
    "\n",
    "  first_tokens = modelo.tokenizer.convert_ids_to_tokens(range(5))\n",
    "  print(f\"- primeiros tokens: {first_tokens}\")\n",
    "\n",
    "  # Taquei o prompt pro LLM. Temperatura controla o quao \"criativo\" o modelo será.\n",
    "  # É preciso setar do_sample para controlar a temperatura\n",
    "  output = modelo(prompt, do_sample=True, temperature=0.7)\n",
    "  print(f\"- output do modelo: {output}\")\n",
    "  print(f\"- texto gerado efetivamente: {output[0]['generated_text']}\")\n",
    "\n",
    "  print(\"=\"*80)\n",
    "  return\n",
    "\n",
    "analisa_e_processa_prompt(gerbo, \"Once upon a time\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
